"""
vectors.similarity
==================

Find similar vectors using a FAISS index. The methodology
assumes very high-dimensionality vectors which, almost by
definition, tend to occupy very sparse spaces. For this reason, 
it is the L1 (Manhattan distance) metric which is used by 
default, although this can be overwritten. The main use cases 
are for finding near duplicates, and otherwise "contextually"
similar vectors.

A score is returned, which is defined relative to the `k_large`
nearest neighbours. To understand this, you should consider
that we make no assumptions about:

a) the lumpiness (i.e. density) of the vector space, in that 
   we assume that the space may be arbitarily lumpy; and
b) a consistent definition of the "physical" interpretation 
   of the density of any particular region of the vector space.
   This is to say that the space is not assumed to be flat.

Sampling only the `k_large` nearest neighbours therefore
means that each region local to each vector is treated
independently from one another. This then allows for a consistent
definition of similarity, in terms of the mean distance
of the `k_large` nearest neighbours to a "query" vector,
such that vectors close to the mean distance have a score of zero
and the vectors close to the query vector have a score of one
(noting, of course, that negative scores are possible but
uninteresting by definition).

Clearly if `k_large` is too large then the assumption a) falls apart
and if too small then assumption b) falls apart. `k_large` should
ideally be tuned to be roughly the lower limit of how the
neighbourhood of "contextually" similar vectors is. One might expect,
in the case of a million text documents, that such "soft clusters" 
would have at least 1000 members and so the default value of 
`k_large` has been set at 1000.
"""

from nesta.packages.vectors.read import download_vectors
import faiss


def find_similar_vectors(data, ids, k=20, k_large=1000,
                         n_clusters=250,
                         metric=faiss.METRIC_L1, score_threshold=0.5):
    """Returns a lookup of similar vectors, by ID.
    Similarity is determined by the given metric parameter. For high-dim
    vectors, such as those generated by BERT transformers
    this should be faiss.METRIC_L1. Explicitly, documents with

        (mean(D_large) - D) / mean(D_large) > duplicate_threshold

    are counted as "duplicates" of each other, where D_large is a vector of
    distances of the k_large nearest neighbours, and D is a vector of
    distances of the k nearest neighbours.

    Args:
        data (np.array): An array of vectors.
        ids (np.array): An array of id fields.
        k (int): The maximum number of duplicates that can be found. (default=10)
        k_large (int): The sample size of "background" neighbour documents, for
                       quantifying similarity. The larger this number is, the
                       looser the definition is of "near" duplicates, and
                       so more results will be returned; although it will
                       have no impact on the number of exact duplicates.
                       (default=1000)
        metric (faiss.METRIC*): The distance metric for faiss to use.
                                (default=faiss.METRIC_L2)
        score_threshold (float): See above for definition. (default=0.5)
    """
    n, d = data.shape
    k = n if k > n else k
    k_large = n if k_large > n else k_large
    n_clusters = n if n < n_clusters else n_clusters
    quantizer = faiss.IndexFlat(d, metric)
    index = faiss.IndexIVFFlat(quantizer, d, n_clusters)
    index.train(data)
    index.add(data)
    
    # Make an expansive search to determine the base level of 
    # similarity in this space as the mean similarity of documents
    # in the close vicinity
    index.nprobe = 100
    D, I = index.search(data, k_large)
    base_similarity = D.mean(axis=1)  # Calculate the mean distance

    # Now subset only the top k results
    D = D[:,:k]  # Distances
    I = I[:,:k]  # Indexes of the k results

    # Extract similar vectors
    similar_vectors = {}
    for _id, all_ids, sims, base in zip(ids, ids[I], D, base_similarity):
        _id = str(_id)  # FAISS returns ids as strings
        scores = (base - sims) / base
        over_threshold = scores > score_threshold
        # If no similar results, noting that the query vector is always
        # found so there will always be one result
        if over_threshold.sum() <= 1:
            continue
        results = {i: float(s) for i, s in zip(all_ids, scores)
                   if s > score_threshold  # Ignore low scores
                   and _id != i  # Ignore the query vector itself
                   and i not in similar_vectors}  # Don't duplicate results
        # Possible that there are no similar vectors, 
        # depending on the score_threshold
        if len(results) == 0:
            continue
        similar_vectors[_id] = results
    return similar_vectors


def generate_duplicate_links(orm, id_field, database, k=20, k_large=1000,
                             n_clusters=250,
                             metric=faiss.METRIC_L1, duplicate_threshold=0.5,
                             read_chunksize=10000, read_max_chunks=None):
    """Convenience method finding duplicate text via embeddings
    in the database.

    All vectors are read into memory and then indexed with FAISS before
    selecting only very similar vectors as being "duplicates".

    Similarity is determined by the given metric parameter. For high-dim
    vectors, such as those generated by BERT transformers
    this should be faiss.METRIC_L1. Explicitly, documents with

        (mean(D_large) - D) / mean(D_large) > duplicate_threshold

    are counted as "duplicates" of each other, where D_large is a vector of
    distances of the k_large nearest neighbours, and D is a vector of
    distances of the k nearest neighbours.

    Args:
        orm (sqlalchemy.Base): A SqlAlchemy ORM, representing the table of vectors
        id_field (str): The name of the id field in the ORM.
        database (str): The name of the MySQL database ("dev" or "production")
        k (int): The maximum number of duplicates that can be found. (default=10)
        k_large (int): The sample size of "background" neighbour documents, for
                       quantifying similarity. The larger this number is, the
                       looser the definition is of "near" duplicates, and
                       so more results will be returned; although it will
                       have no impact on the number of exact duplicates.
                       (default=1000)
        metric (faiss.METRIC*): The distance metric for faiss to use.
                                (default=faiss.METRIC_L2)
        duplicate_threshold (float): See above for definition. (default=0.9)
        read_chunksize (int): The number of rows to read from the database
                              at a time. You might need to reduce this
                              number if running on WiFi. (default=10000)
        read_max_chunks (int): The maximum number of chunks to
                               read from the database (e.g. if in testing mode).
                               If set to None (default) then all data will be read.
                               (default=None)

    Returns:
        links (json): Rows containing the ids of matching documents
                      and their match weight.
    """
    # Read the data
    data, ids = download_vectors(orm=orm, id_field=id_field, database=database,
                                 chunksize=read_chunksize, 
                                 max_chunks=read_max_chunks)
    # Find all sets of similar vectors
    similar_vectors = find_similar_vectors(data=data, ids=ids, k=k,
                                           k_large=k_large, metric=metric,
                                           n_clusters=n_clusters,
                                           score_threshold=duplicate_threshold)
    # Clean up
    del data
    del ids
    # Structure the output for ingestion to the database as a link table
    links = [{f"{id_field}_1": _id1, f"{id_field}_2": _id2, "weight": weight}
             for _id1, sims in similar_vectors.items()
             for _id2, weight in sims.items()]
    return links
